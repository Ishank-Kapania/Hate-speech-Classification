{"cells":[{"cell_type":"markdown","metadata":{"id":"p6yDVpK88kHE"},"source":["# Baseline for HASOC\n","Code Reference: https://github.com/mdabashar/QutNocturnal-Hasoc2019/blob/master/CNN%20-%20Hate%20Speech%20and%20Offensive%20Content%20Identification%20In%20Hindi.ipynb"]},{"cell_type":"markdown","metadata":{"id":"pIpXLVwG8kHF"},"source":["# Import common libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tjbElOct8kHF"},"outputs":[],"source":["\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","import tensorflow as tf\n","import numpy as np\n","import pandas as pd\n","import random as rn\n","import spacy\n","import re\n","import html\n","import simplemma"]},{"cell_type":"markdown","metadata":{"id":"YbKa22NB8kHG"},"source":["# Initialise Random variables and Tensor Board"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qjUn-xCY8kHG"},"outputs":[],"source":["#SEED = 100\n","SEED = 123\n","\n","#reference: https://keras.io/getting-started/faq/#how-can-i-obtain-reproducible-results-using-keras-during-development\n","# The below is necessary in Python 3.2.3 onwards to\n","# have reproducible behavior for certain hash-based operations.\n","# See these references for further details:\n","# https://docs.python.org/3.4/using/cmdline.html#envvar-PYTHONHASHSEED\n","# https://github.com/keras-team/keras/issues/2280#issuecomment-306959926\n","\n","import os\n","os.environ['PYTHONHASHSEED'] = '0'\n","\n","# The below is necessary for starting Numpy generated random numbers\n","# in a well-defined initial state.\n","\n","np.random.seed(SEED)\n","\n","# The below is necessary for starting core Python generated random numbers\n","# in a well-defined state.\n","\n","rn.seed(SEED)\n","\n","# Force TensorFlow to use single thread.\n","# Multiple threads are a potential source of\n","# non-reproducible results.\n","# For further details, see: https://stackoverflow.com/questions/42022950/which-seeds-have-to-be-set-where-to-realize-100-reproducibility-of-training-res\n","\n","session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n","\n","from keras import backend as K\n","\n","# The below tf.set_random_seed() will make random number generation\n","# in the TensorFlow backend have a well-defined initial state.\n","# For further details, see: https://www.tensorflow.org/api_docs/python/tf/set_random_seed\n","tf.random.set_seed(SEED)\n","sess = tf.compat.v1.Session(config=session_conf)\n","tf.compat.v1.keras.backend.set_session(sess)\n","\n","# Rest of code follows ..."]},{"cell_type":"markdown","metadata":{"id":"xoygmZOG8kHH"},"source":["# Preprocessing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ylADLSBc8kHH"},"outputs":[],"source":["re1 = re.compile(r' +')\n","\n","def textFixup(aText):\n","    aText = aText.replace('#39;', \"'\").replace('amp;', '&').replace('#146;', \"'\").replace(\n","        'nbsp;', ' ').replace('#36;', '$').replace('\\\\n', \"\\n\").replace('quot;', \"'\").replace(\n","        '<br />', \"\\n\").replace('\\\\\"', '\"').replace('<unk>','u_n').replace(' @.@ ','.').replace(\n","        ' @-@ ', '-').replace('\\\\', ' \\\\ ').replace('â€™', \"'\")\n","    return re1.sub(' ', html.unescape(aText))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KLAUHNQ18kHI","outputId":"51f2acca-1d62-4181-af72-16dd80450a21"},"outputs":[{"name":"stdout","output_type":"stream","text":["2023-10-02 13:57:12.594858: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","2023-10-02 13:57:12.596475: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n","2023-10-02 13:57:12.618110: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2023-10-02 13:57:12.618144: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2023-10-02 13:57:12.618161: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2023-10-02 13:57:12.622504: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n","2023-10-02 13:57:12.622664: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2023-10-02 13:57:13.167899: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","Collecting xx-ent-wiki-sm==3.6.0\n","  Downloading https://github.com/explosion/spacy-models/releases/download/xx_ent_wiki_sm-3.6.0/xx_ent_wiki_sm-3.6.0-py3-none-any.whl (11.1 MB)\n","\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.1/11.1 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0mm\n","\u001b[?25hRequirement already satisfied: spacy<3.7.0,>=3.6.0 in /home/gaurang/anaconda3/lib/python3.11/site-packages (from xx-ent-wiki-sm==3.6.0) (3.6.1)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /home/gaurang/anaconda3/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.0->xx-ent-wiki-sm==3.6.0) (3.0.12)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/gaurang/anaconda3/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.0->xx-ent-wiki-sm==3.6.0) (1.0.5)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/gaurang/anaconda3/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.0->xx-ent-wiki-sm==3.6.0) (1.0.10)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/gaurang/anaconda3/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.0->xx-ent-wiki-sm==3.6.0) (2.0.8)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/gaurang/anaconda3/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.0->xx-ent-wiki-sm==3.6.0) (3.0.9)\n","Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /home/gaurang/anaconda3/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.0->xx-ent-wiki-sm==3.6.0) (8.1.12)\n","Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /home/gaurang/anaconda3/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.0->xx-ent-wiki-sm==3.6.0) (1.1.2)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/gaurang/anaconda3/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.0->xx-ent-wiki-sm==3.6.0) (2.4.8)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/gaurang/anaconda3/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.0->xx-ent-wiki-sm==3.6.0) (2.0.10)\n","Requirement already satisfied: typer<0.10.0,>=0.3.0 in /home/gaurang/anaconda3/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.0->xx-ent-wiki-sm==3.6.0) (0.9.0)\n","Requirement already satisfied: pathy>=0.10.0 in /home/gaurang/anaconda3/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.0->xx-ent-wiki-sm==3.6.0) (0.10.2)\n","Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /home/gaurang/anaconda3/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.0->xx-ent-wiki-sm==3.6.0) (5.2.1)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/gaurang/anaconda3/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.0->xx-ent-wiki-sm==3.6.0) (4.65.0)\n","Requirement already satisfied: numpy>=1.15.0 in /home/gaurang/anaconda3/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.0->xx-ent-wiki-sm==3.6.0) (1.24.3)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/gaurang/anaconda3/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.0->xx-ent-wiki-sm==3.6.0) (2.29.0)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /home/gaurang/anaconda3/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.0->xx-ent-wiki-sm==3.6.0) (2.4.2)\n","Requirement already satisfied: jinja2 in /home/gaurang/anaconda3/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.0->xx-ent-wiki-sm==3.6.0) (3.1.2)\n","Requirement already satisfied: setuptools in /home/gaurang/anaconda3/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.0->xx-ent-wiki-sm==3.6.0) (67.8.0)\n","Requirement already satisfied: packaging>=20.0 in /home/gaurang/anaconda3/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.0->xx-ent-wiki-sm==3.6.0) (23.0)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/gaurang/anaconda3/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.0->xx-ent-wiki-sm==3.6.0) (3.3.0)\n","Requirement already satisfied: annotated-types>=0.4.0 in /home/gaurang/anaconda3/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->xx-ent-wiki-sm==3.6.0) (0.5.0)\n","Requirement already satisfied: pydantic-core==2.10.1 in /home/gaurang/anaconda3/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->xx-ent-wiki-sm==3.6.0) (2.10.1)\n","Requirement already satisfied: typing-extensions>=4.6.1 in /home/gaurang/anaconda3/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->xx-ent-wiki-sm==3.6.0) (4.6.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /home/gaurang/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->xx-ent-wiki-sm==3.6.0) (2.0.4)\n","Requirement already satisfied: idna<4,>=2.5 in /home/gaurang/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->xx-ent-wiki-sm==3.6.0) (3.4)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/gaurang/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->xx-ent-wiki-sm==3.6.0) (1.26.16)\n","Requirement already satisfied: certifi>=2017.4.17 in /home/gaurang/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->xx-ent-wiki-sm==3.6.0) (2023.5.7)\n","Requirement already satisfied: blis<0.8.0,>=0.7.8 in /home/gaurang/anaconda3/lib/python3.11/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->xx-ent-wiki-sm==3.6.0) (0.7.11)\n","Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/gaurang/anaconda3/lib/python3.11/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->xx-ent-wiki-sm==3.6.0) (0.1.3)\n","Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/gaurang/anaconda3/lib/python3.11/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->xx-ent-wiki-sm==3.6.0) (8.0.4)\n","Requirement already satisfied: MarkupSafe>=2.0 in /home/gaurang/anaconda3/lib/python3.11/site-packages (from jinja2->spacy<3.7.0,>=3.6.0->xx-ent-wiki-sm==3.6.0) (2.1.1)\n","Installing collected packages: xx-ent-wiki-sm\n","Successfully installed xx-ent-wiki-sm-3.6.0\n","\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the package via spacy.load('xx_ent_wiki_sm')\n"]}],"source":["!python -m spacy download xx_ent_wiki_sm"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"irCcB2QX8kHJ"},"outputs":[],"source":["#! /usr/bin/env python3.1\n","''' Lightweight Hindi stemmer\n","Copyright © 2010 Luís Gomes <luismsgomes@gmail.com>.\n","\n","Implementation of algorithm described in\n","\n","    A Lightweight Stemmer for Hindi\n","    Ananthakrishnan Ramanathan and Durgesh D Rao\n","    http://computing.open.ac.uk/Sites/EACLSouthAsia/Papers/p6-Ramanathan.pdf\n","\n","    @conference{ramanathan2003lightweight,\n","      title={{A lightweight stemmer for Hindi}},\n","      author={Ramanathan, A. and Rao, D.},\n","      booktitle={Workshop on Computational Linguistics for South-Asian Languages, EACL},\n","      year={2003}\n","    }\n","\n","Ported from HindiStemmer.java, part of of Lucene.\n","'''\n","\n","suffixes = {\n","    1: [\"ो\", \"े\", \"ू\", \"ु\", \"ी\", \"ि\", \"ा\"],\n","    2: [\"कर\", \"ाओ\", \"िए\", \"ाई\", \"ाए\", \"ने\", \"नी\", \"ना\", \"ते\", \"ीं\", \"ती\", \"ता\", \"ाँ\", \"ां\", \"ों\", \"ें\"],\n","    3: [\"ाकर\", \"ाइए\", \"ाईं\", \"ाया\", \"ेगी\", \"ेगा\", \"ोगी\", \"ोगे\", \"ाने\", \"ाना\", \"ाते\", \"ाती\", \"ाता\", \"तीं\", \"ाओं\", \"ाएं\", \"ुओं\", \"ुएं\", \"ुआं\"],\n","    4: [\"ाएगी\", \"ाएगा\", \"ाओगी\", \"ाओगे\", \"एंगी\", \"ेंगी\", \"एंगे\", \"ेंगे\", \"ूंगी\", \"ूंगा\", \"ातीं\", \"नाओं\", \"नाएं\", \"ताओं\", \"ताएं\", \"ियाँ\", \"ियों\", \"ियां\"],\n","    5: [\"ाएंगी\", \"ाएंगे\", \"ाऊंगी\", \"ाऊंगा\", \"ाइयाँ\", \"ाइयों\", \"ाइयां\"],\n","}\n","\n","def hi_stem(word):\n","    for L in 5, 4, 3, 2, 1:\n","        if len(word) > L + 1:\n","            for suf in suffixes[L]:\n","                if word.endswith(suf):\n","                    return word[:-L]\n","    return word\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dMk2G9YM8kHK"},"outputs":[],"source":["hi_nlp = spacy.load(\"xx_ent_wiki_sm\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yHNTuPTo8kHK"},"outputs":[],"source":["def preprocess_aTweet(tweet):\n","\n","    tweet = tweet.lower()\n","    tweet = textFixup(tweet)\n","\n","\n","    tokens = [simplemma.lemmatize(str(token), lang='hi') for token in hi_nlp(tweet)]\n","\n","    tokens = [hi_stem(t) for t in tokens]\n","\n","    return ' '.join(tokens)"]},{"cell_type":"markdown","metadata":{"id":"dl_-Qlyi8kHK"},"source":["# Loading Data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J8LHuhly8kHL"},"outputs":[],"source":["def load_data_and_labels_csv(fileLoc):\n","    examples = []\n","    labels = []\n","    df = pd.read_csv(fileLoc)\n","    for i in df.index:\n","        examples.append(preprocess_aTweet(df['text'][i]))\n","        if int(df['label_yn'][i])==1:\n","            labels.append(1)\n","        elif int(df['label_yn'][i])==0:\n","            labels.append(0)\n","    return examples, labels\n","\n","X_train, y_train = load_data_and_labels_csv('hasoc_hi_t1_train.csv')\n","\n","X_test, y_test = load_data_and_labels_csv('hasoc_hi_t1_test.csv')\n","\n","\n","\n","ytrain = np.array(y_train)\n","ytest = np.array(y_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OJcaOIQc8kHL"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"L6iyYHOj8kHL"},"source":["# Transforming data suitable for model format"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AXKSgi2A8kHL"},"outputs":[],"source":["from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","num_words = 100000\n","tokenizer = Tokenizer(num_words=num_words)\n","tokenizer.fit_on_texts(X_train)\n","xtrain = tokenizer.texts_to_sequences(X_train)\n","maxlen = max(map(lambda x: len(x),xtrain))\n","xtrain = pad_sequences(xtrain, maxlen=maxlen)\n","\n","xtest = tokenizer.texts_to_sequences(X_test)\n","xtest = pad_sequences(xtest, maxlen=maxlen)"]},{"cell_type":"markdown","metadata":{"id":"nBR8ju8L8kHL"},"source":["# Loading word embedding and mapping data to that word embedding"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7ByfiwNX8kHL"},"outputs":[],"source":["from gensim.models import KeyedVectors\n","model_ug_cbow = KeyedVectors.load('hi/vectors.txt')\n","\n","embeddings_index = {}\n","for w in model_ug_cbow.wv.index_to_key:\n","    embeddings_index[w] = model_ug_cbow.wv[w]\n","\n","embedding_matrix = np.zeros((num_words, 200))\n","for word, i in tokenizer.word_index.items():\n","    if i >= num_words:\n","        continue\n","    embedding_vector = embeddings_index.get(word)\n","    if embedding_vector is not None:\n","        embedding_matrix[i] = embedding_vector"]},{"cell_type":"markdown","metadata":{"id":"-0RdiNJk8kHL"},"source":["# Creating CNN model and training it for 10 epoc"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hceu51hx8kHL","outputId":"1da97951-ec8b-4a94-8881-59f022ad502b"},"outputs":[{"name":"stdout","output_type":"stream","text":["loading word vectors for lang is hi 144\n","loading word vectors for lang is hi\n","loading word vectors for lang is hi (None, 144, 200)\n","Epoch 1/10\n","Epoch 2/10\n","Epoch 3/10\n","Epoch 4/10\n","Epoch 5/10\n","Epoch 6/10\n","Epoch 7/10\n","Epoch 8/10\n","Epoch 9/10\n","Epoch 10/10\n"]},{"data":{"text/plain":["<keras.src.callbacks.History at 0x7f386fb47310>"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["from keras.layers import Dense, Dropout\n","from keras.layers import Embedding\n","from keras.layers import Conv1D, GlobalMaxPooling1D\n","from keras.layers import Input, concatenate, Activation\n","from keras.models import Model\n","\n","def create_cnn_model():\n","    tweet_input = Input(shape=(maxlen,), dtype='int32')\n","    print('loading word vectors for lang is hi',maxlen)\n","\n","    print('loading word vectors for lang is hi')\n","    tweet_encoder = Embedding(num_words, 200, weights=[embedding_matrix], input_length=maxlen, trainable=True)(tweet_input)\n","\n","    tweet_encoder = Dropout(0.5)(tweet_encoder)\n","    print('loading word vectors for lang is hi',tweet_encoder.shape)\n","    bigram_branch = Conv1D(filters=128, kernel_size=3, padding='valid', activation='relu', strides=1)(tweet_encoder)\n","    bigram_branch = GlobalMaxPooling1D(data_format='channels_first')(bigram_branch)\n","    bigram_branch = Dropout(0.5)(bigram_branch)\n","\n","    trigram_branch = Conv1D(filters=256, kernel_size=4, padding='valid', activation='relu', strides=1,)(tweet_encoder)\n","    trigram_branch = GlobalMaxPooling1D(data_format='channels_first')(trigram_branch)\n","    trigram_branch = Dropout(0.2)(trigram_branch)\n","\n","    fourgram_branch = Conv1D(filters=512, kernel_size=5, padding='valid', activation='relu', strides=1,)(tweet_encoder)\n","    fourgram_branch = GlobalMaxPooling1D(data_format='channels_first')(fourgram_branch)\n","    fourgram_branch = Dropout(0.2)(fourgram_branch)\n","\n","    merged = concatenate([bigram_branch, trigram_branch, fourgram_branch], axis=1)\n","\n","    merged = Dense(256, activation='relu')(merged)\n","    merged = Dropout(0.5)(merged)\n","\n","    merged = Dense(1)(merged)\n","    output = Activation('sigmoid')(merged)\n","\n","    model = Model(inputs=[tweet_input], outputs=[output])\n","    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","    #model.summary()\n","    return model\n","\n","cnn_model = create_cnn_model()\n","cnn_model.fit(xtrain, ytrain, epochs=10, batch_size=32, verbose=3)"]},{"cell_type":"markdown","metadata":{"id":"ua4Y8UQg8kHL"},"source":["# Evaluating the model with test dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"XBg2qxj48kHL","outputId":"bfbab5cd-2e70-4fbb-9e5b-6ea485661a06"},"outputs":[{"name":"stdout","output_type":"stream","text":["42/42 [==============================] - 2s 41ms/step\n","Accuracy\t0.8034901365705615\n","Precision\t0.7485632183908046\n","Recall\t0.8611570247933884\n","f-measure\t0.8009223674096849\n","cohen_kappa_score\t0.6087801613187891\n","auc\t0.8078576147809859\n","roc_auc\t0.8078576147809859\n"]}],"source":["from sklearn.metrics import cohen_kappa_score\n","from sklearn.metrics import roc_curve, auc, roc_auc_score\n","from sklearn.metrics import classification_report\n","\n","p = cnn_model.predict(xtest,verbose=1)\n","\n","predicted = [int(round(x[0])) for x in p]\n","predicted = np.array(predicted)\n","actual = ytest\n","\n","tp = np.count_nonzero(predicted * actual)\n","tn = np.count_nonzero((predicted - 1) * (actual - 1))\n","fp = np.count_nonzero(predicted * (actual - 1))\n","fn = np.count_nonzero((predicted - 1) * actual)\n","\n","\n","\n","accuracy = (tp + tn) / (tp + fp + fn + tn)\n","precision = tp / (tp + fp)\n","recall = tp / (tp + fn)\n","fmeasure = (2 * precision * recall) / (precision + recall)\n","cohen_kappa_score = cohen_kappa_score(predicted, actual)\n","false_positive_rate, true_positive_rate, thresholds = roc_curve(actual, predicted)\n","auc_val = auc(false_positive_rate, true_positive_rate)\n","roc_auc_val = roc_auc_score(actual, predicted)\n","\n","print('Accuracy\\t' + str(accuracy))\n","print('Precision\\t' + str(precision))\n","print('Recall\\t' + str(recall))\n","print('f-measure\\t' + str(fmeasure))\n","print('cohen_kappa_score\\t' + str(cohen_kappa_score))\n","print('auc\\t' + str(auc_val))\n","print('roc_auc\\t' + str(roc_auc_val))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Df_UHCub8kHM"},"outputs":[],"source":["model_name = 'CNN'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ddfJG-at8kHM"},"outputs":[],"source":["import datetime\n","now = datetime.datetime.now()\n","\n","out_string = '=========='+str(now)+'==============\\n'\n","#out_string += 'Language:\\t'+ lang+'\\n'\n","out_string += 'Dataset:\\t' + dataset_name + '\\n'\n","out_string += 'Task:\\t' + task + '\\n'\n","out_string += str('Model Name:\\t' + model_name+'\\n')\n","out_string += '-------------------------------------------------' + '\\n'\n","\n","out_string += 'Total Samples:\\t' + str(len(actual)) + '\\n'\n","out_string += 'Positive Samples:\\t' + str(sum(actual)) + '\\n'\n","out_string += 'Negative Samples:\\t' + str(len(actual)-sum(actual)) + '\\n'\n","\n","out_string += 'True Positive:\\t' + str(tp) + '\\n'\n","out_string += 'True Negative:\\t' + str(tn) + '\\n'\n","out_string += 'False Positive:\\t' + str(fp) + '\\n'\n","out_string += 'False Negative:\\t' + str(fn) + '\\n'\n","\n","out_string += 'Accuracy:\\t' + str(accuracy) + '\\n'\n","out_string += 'Precision:\\t' + str(precision) + '\\n'\n","out_string += 'Recall:\\t' + str(recall) + '\\n'\n","out_string += 'F-measure:\\t' + str(fmeasure) + '\\n'\n","out_string += 'Cohen_Kappa_Score:\\t' + str(cohen_kappa_score) + '\\n'\n","out_string += 'AUC:\\t' + str(auc_val) + '\\n'\n","out_string += 'ROC_AUC:\\t' + str(roc_auc_val) + '\\n'\n","out_string += '\\n'\n","out_string += classification_report(actual, predicted)\n","out_string += '\\n'\n","print(out_string)\n","with open(model_name+'.txt', 'a+') as FO:\n","    FO.write(out_string)"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.3"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}