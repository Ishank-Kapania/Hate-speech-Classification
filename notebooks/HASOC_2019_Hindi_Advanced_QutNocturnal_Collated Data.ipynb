{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZKxdSPVR6WD"
      },
      "source": [
        "# Baseline for HASOC\n",
        "Code Reference: https://github.com/mdabashar/QutNocturnal-Hasoc2019/blob/master/CNN%20-%20Hate%20Speech%20and%20Offensive%20Content%20Identification%20In%20Hindi.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training the CNN model on collated data\n",
        "- Checking accuracy on collated test data\n",
        "- Checking accuracy on original HASOC test dataset"
      ],
      "metadata": {
        "id": "w8XIq3zrdoAv"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pwY1g__sVTfE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sqq58sBbVUQh",
        "outputId": "d7e67c42-09ca-40a8-f2e3-ae16e0bd6fa3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install simplemma"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "90GucCveSQ13",
        "outputId": "047cbefa-7d5a-4ae5-bb7e-efb769be45f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting simplemma\n",
            "  Downloading simplemma-0.9.1-py3-none-any.whl (75.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.5/75.5 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: simplemma\n",
            "Successfully installed simplemma-0.9.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2w6nKYdR6WE"
      },
      "source": [
        "# Import common libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PGXbnzt8R6WF"
      },
      "outputs": [],
      "source": [
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random as rn\n",
        "import spacy\n",
        "import re\n",
        "import html\n",
        "import simplemma"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "85O5QgA0R6WG"
      },
      "outputs": [],
      "source": [
        "df=pd.DataFrame(pd.read_csv('hindi/hate-classifier-hindi.csv'))\n",
        "train = df[df[\"uid\"].str.contains(\"train\")]\n",
        "val = df[df[\"uid\"].str.contains(\"val\")]\n",
        "test = df[df[\"uid\"].str.contains(\"test\")]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tX1iQDTuR6WG",
        "outputId": "fd8055ac-4541-49fe-fb26-e1adeb7b42a6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(29317, 3)"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TQXu_flCR6WG",
        "outputId": "8dcc4672-d5e1-477b-915f-40bee8f4eca0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(7348, 3)"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "val.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eI3ZW9_rR6WH",
        "outputId": "7bfcfcc2-e74f-4fdc-c34f-ae34267c7005"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(7992, 3)"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3hWuwcvrR6WH",
        "outputId": "94d555d7-9c45-4b95-a547-8b9150108290"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(36665, 6)"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train = pd.concat([train, val], axis= 1)\n",
        "train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9vBR8haGR6WI"
      },
      "outputs": [],
      "source": [
        "train.to_csv('hindi_train.csv')\n",
        "test.to_csv('hindi_test.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aVSf9ErjR6WI"
      },
      "source": [
        "# Initialise Random variables and Tensor Board"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OVTLEugzR6WJ"
      },
      "outputs": [],
      "source": [
        "#SEED = 100\n",
        "SEED = 123\n",
        "\n",
        "#reference: https://keras.io/getting-started/faq/#how-can-i-obtain-reproducible-results-using-keras-during-development\n",
        "# The below is necessary in Python 3.2.3 onwards to\n",
        "# have reproducible behavior for certain hash-based operations.\n",
        "# See these references for further details:\n",
        "# https://docs.python.org/3.4/using/cmdline.html#envvar-PYTHONHASHSEED\n",
        "# https://github.com/keras-team/keras/issues/2280#issuecomment-306959926\n",
        "\n",
        "import os\n",
        "os.environ['PYTHONHASHSEED'] = '0'\n",
        "\n",
        "# The below is necessary for starting Numpy generated random numbers\n",
        "# in a well-defined initial state.\n",
        "\n",
        "np.random.seed(SEED)\n",
        "\n",
        "# The below is necessary for starting core Python generated random numbers\n",
        "# in a well-defined state.\n",
        "\n",
        "rn.seed(SEED)\n",
        "\n",
        "# Force TensorFlow to use single thread.\n",
        "# Multiple threads are a potential source of\n",
        "# non-reproducible results.\n",
        "# For further details, see: https://stackoverflow.com/questions/42022950/which-seeds-have-to-be-set-where-to-realize-100-reproducibility-of-training-res\n",
        "\n",
        "session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
        "\n",
        "from keras import backend as K\n",
        "\n",
        "# The below tf.set_random_seed() will make random number generation\n",
        "# in the TensorFlow backend have a well-defined initial state.\n",
        "# For further details, see: https://www.tensorflow.org/api_docs/python/tf/set_random_seed\n",
        "tf.random.set_seed(SEED)\n",
        "sess = tf.compat.v1.Session(config=session_conf)\n",
        "tf.compat.v1.keras.backend.set_session(sess)\n",
        "\n",
        "# Rest of code follows ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fFOtJcR5R6WJ"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3q5e1oTDR6WK"
      },
      "outputs": [],
      "source": [
        "re1 = re.compile(r' +')\n",
        "\n",
        "def textFixup(aText):\n",
        "    aText = aText.replace('#39;', \"'\").replace('amp;', '&').replace('#146;', \"'\").replace(\n",
        "        'nbsp;', ' ').replace('#36;', '$').replace('\\\\n', \"\\n\").replace('quot;', \"'\").replace(\n",
        "        '<br />', \"\\n\").replace('\\\\\"', '\"').replace('<unk>','u_n').replace(' @.@ ','.').replace(\n",
        "        ' @-@ ', '-').replace('\\\\', ' \\\\ ').replace('â€™', \"'\")\n",
        "    return re1.sub(' ', html.unescape(aText))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5IwJ_xxBR6WK",
        "outputId": "9f616fde-3737-4b65-fd15-252fc965b94d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-11-16 09:01:40.613817: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-16 09:01:40.613877: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-16 09:01:40.613915: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-16 09:01:42.898127: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Collecting xx-ent-wiki-sm==3.6.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/xx_ent_wiki_sm-3.6.0/xx_ent_wiki_sm-3.6.0-py3-none-any.whl (11.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.1/11.1 MB\u001b[0m \u001b[31m47.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.7.0,>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from xx-ent-wiki-sm==3.6.0) (3.6.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->xx-ent-wiki-sm==3.6.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->xx-ent-wiki-sm==3.6.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->xx-ent-wiki-sm==3.6.0) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->xx-ent-wiki-sm==3.6.0) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->xx-ent-wiki-sm==3.6.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->xx-ent-wiki-sm==3.6.0) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->xx-ent-wiki-sm==3.6.0) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->xx-ent-wiki-sm==3.6.0) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->xx-ent-wiki-sm==3.6.0) (2.0.10)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->xx-ent-wiki-sm==3.6.0) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->xx-ent-wiki-sm==3.6.0) (0.10.3)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->xx-ent-wiki-sm==3.6.0) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->xx-ent-wiki-sm==3.6.0) (4.66.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->xx-ent-wiki-sm==3.6.0) (1.23.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->xx-ent-wiki-sm==3.6.0) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->xx-ent-wiki-sm==3.6.0) (1.10.13)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->xx-ent-wiki-sm==3.6.0) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->xx-ent-wiki-sm==3.6.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->xx-ent-wiki-sm==3.6.0) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->xx-ent-wiki-sm==3.6.0) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->xx-ent-wiki-sm==3.6.0) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->xx-ent-wiki-sm==3.6.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->xx-ent-wiki-sm==3.6.0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->xx-ent-wiki-sm==3.6.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->xx-ent-wiki-sm==3.6.0) (2023.7.22)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->xx-ent-wiki-sm==3.6.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->xx-ent-wiki-sm==3.6.0) (0.1.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->xx-ent-wiki-sm==3.6.0) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.7.0,>=3.6.0->xx-ent-wiki-sm==3.6.0) (2.1.3)\n",
            "Installing collected packages: xx-ent-wiki-sm\n",
            "Successfully installed xx-ent-wiki-sm-3.6.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('xx_ent_wiki_sm')\n"
          ]
        }
      ],
      "source": [
        "!python -m spacy download xx_ent_wiki_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cGRw9zPOR6WK"
      },
      "outputs": [],
      "source": [
        "#! /usr/bin/env python3.1\n",
        "''' Lightweight Hindi stemmer\n",
        "Copyright © 2010 Luís Gomes <luismsgomes@gmail.com>.\n",
        "\n",
        "Implementation of algorithm described in\n",
        "\n",
        "    A Lightweight Stemmer for Hindi\n",
        "    Ananthakrishnan Ramanathan and Durgesh D Rao\n",
        "    http://computing.open.ac.uk/Sites/EACLSouthAsia/Papers/p6-Ramanathan.pdf\n",
        "\n",
        "    @conference{ramanathan2003lightweight,\n",
        "      title={{A lightweight stemmer for Hindi}},\n",
        "      author={Ramanathan, A. and Rao, D.},\n",
        "      booktitle={Workshop on Computational Linguistics for South-Asian Languages, EACL},\n",
        "      year={2003}\n",
        "    }\n",
        "\n",
        "Ported from HindiStemmer.java, part of of Lucene.\n",
        "'''\n",
        "\n",
        "suffixes = {\n",
        "    1: [\"ो\", \"े\", \"ू\", \"ु\", \"ी\", \"ि\", \"ा\"],\n",
        "    2: [\"कर\", \"ाओ\", \"िए\", \"ाई\", \"ाए\", \"ने\", \"नी\", \"ना\", \"ते\", \"ीं\", \"ती\", \"ता\", \"ाँ\", \"ां\", \"ों\", \"ें\"],\n",
        "    3: [\"ाकर\", \"ाइए\", \"ाईं\", \"ाया\", \"ेगी\", \"ेगा\", \"ोगी\", \"ोगे\", \"ाने\", \"ाना\", \"ाते\", \"ाती\", \"ाता\", \"तीं\", \"ाओं\", \"ाएं\", \"ुओं\", \"ुएं\", \"ुआं\"],\n",
        "    4: [\"ाएगी\", \"ाएगा\", \"ाओगी\", \"ाओगे\", \"एंगी\", \"ेंगी\", \"एंगे\", \"ेंगे\", \"ूंगी\", \"ूंगा\", \"ातीं\", \"नाओं\", \"नाएं\", \"ताओं\", \"ताएं\", \"ियाँ\", \"ियों\", \"ियां\"],\n",
        "    5: [\"ाएंगी\", \"ाएंगे\", \"ाऊंगी\", \"ाऊंगा\", \"ाइयाँ\", \"ाइयों\", \"ाइयां\"],\n",
        "}\n",
        "\n",
        "def hi_stem(word):\n",
        "    for L in 5, 4, 3, 2, 1:\n",
        "        if len(word) > L + 1:\n",
        "            for suf in suffixes[L]:\n",
        "                if word.endswith(suf):\n",
        "                    return word[:-L]\n",
        "    return word\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "---8OI_WR6WK"
      },
      "outputs": [],
      "source": [
        "hi_nlp = spacy.load(\"xx_ent_wiki_sm\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QUCK3-lYR6WL"
      },
      "outputs": [],
      "source": [
        "def preprocess_aTweet(tweet):\n",
        "    tweet = tweet.lower()\n",
        "    tweet = textFixup(tweet)\n",
        "\n",
        "\n",
        "    tokens = [simplemma.lemmatize(str(token), lang='hi') for token in hi_nlp(tweet)]\n",
        "\n",
        "    tokens = [hi_stem(t) for t in tokens]\n",
        "\n",
        "    return ' '.join(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1DMmtbZkR6WL"
      },
      "source": [
        "# Loading Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tN4SJUj_R6WL"
      },
      "outputs": [],
      "source": [
        "def load_data_and_labels_csv(fileLoc):\n",
        "    examples = []\n",
        "    labels = []\n",
        "    df = pd.read_csv(fileLoc)\n",
        "    for i in df.index:\n",
        "        examples.append(preprocess_aTweet(df['text'].astype(str)[i]))\n",
        "        if int(df['label_yn'].fillna(0)[i])==1:\n",
        "            labels.append(1)\n",
        "        elif int(df['label_yn'].fillna(0)[i])==0:\n",
        "            labels.append(0)\n",
        "    return examples, labels\n",
        "\n",
        "X_train, y_train = load_data_and_labels_csv('/content/drive/MyDrive/IRE/hindi_train.csv')\n",
        "\n",
        "X_test, y_test = load_data_and_labels_csv('/content/drive/MyDrive/IRE/hindi_test.csv')\n",
        "\n",
        "\n",
        "ytrain = np.array(y_train)\n",
        "ytest = np.array(y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MZJaSwqXR6WL"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCbrIm7kR6WL"
      },
      "source": [
        "# Transforming data suitable for model format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OGpmDJ3TR6WL"
      },
      "outputs": [],
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "num_words = 100000\n",
        "tokenizer = Tokenizer(num_words=num_words)\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "xtrain = tokenizer.texts_to_sequences(X_train)\n",
        "maxlen = max(map(lambda x: len(x),xtrain))\n",
        "xtrain = pad_sequences(xtrain, maxlen=maxlen)\n",
        "\n",
        "xtest = tokenizer.texts_to_sequences(X_test)\n",
        "xtest = pad_sequences(xtest, maxlen=maxlen)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ci9RCFxqR6WM"
      },
      "source": [
        "# Loading word embedding and mapping data to that word embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TZqVh3VzR6WM"
      },
      "outputs": [],
      "source": [
        "from gensim.models import KeyedVectors\n",
        "model_ug_cbow = KeyedVectors.load('/content/drive/MyDrive/IRE/hi/vectors.txt')\n",
        "\n",
        "embeddings_index = {}\n",
        "for w in model_ug_cbow.wv.index_to_key:\n",
        "    embeddings_index[w] = model_ug_cbow.wv[w]\n",
        "\n",
        "embedding_matrix = np.zeros((num_words, 200))\n",
        "for word, i in tokenizer.word_index.items():\n",
        "    if i >= num_words:\n",
        "        continue\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0Cs3oeUR6WM"
      },
      "source": [
        "# Creating CNN model and training it for 10 epoc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1AiIdN0YR6WM",
        "outputId": "4598f789-abc2-492d-d654-1b600737f1fd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "Epoch 2/10\n",
            "Epoch 3/10\n",
            "Epoch 4/10\n",
            "Epoch 5/10\n",
            "Epoch 6/10\n",
            "Epoch 7/10\n",
            "Epoch 8/10\n",
            "Epoch 9/10\n",
            "Epoch 10/10\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x78b3ac1a64d0>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "from keras.layers import Dense, Dropout\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import Conv1D, GlobalMaxPooling1D\n",
        "from keras.layers import Input, concatenate, Activation\n",
        "from keras.models import Model\n",
        "\n",
        "def create_cnn_model():\n",
        "    tweet_input = Input(shape=(maxlen,), dtype='int32')\n",
        "\n",
        "    tweet_encoder = Embedding(num_words, 200, weights=[embedding_matrix], input_length=maxlen, trainable=True)(tweet_input)\n",
        "\n",
        "    tweet_encoder = Dropout(0.5)(tweet_encoder)\n",
        "    bigram_branch = Conv1D(filters=128, kernel_size=3, padding='valid', activation='relu', strides=1)(tweet_encoder)\n",
        "    bigram_branch = GlobalMaxPooling1D(data_format='channels_first')(bigram_branch)\n",
        "    bigram_branch = Dropout(0.5)(bigram_branch)\n",
        "\n",
        "    trigram_branch = Conv1D(filters=256, kernel_size=4, padding='valid', activation='relu', strides=1,)(tweet_encoder)\n",
        "    trigram_branch = GlobalMaxPooling1D(data_format='channels_first')(trigram_branch)\n",
        "    trigram_branch = Dropout(0.2)(trigram_branch)\n",
        "\n",
        "    fourgram_branch = Conv1D(filters=512, kernel_size=5, padding='valid', activation='relu', strides=1,)(tweet_encoder)\n",
        "    fourgram_branch = GlobalMaxPooling1D(data_format='channels_first')(fourgram_branch)\n",
        "    fourgram_branch = Dropout(0.2)(fourgram_branch)\n",
        "\n",
        "    merged = concatenate([bigram_branch, trigram_branch, fourgram_branch], axis=1)\n",
        "\n",
        "    merged = Dense(256, activation='relu')(merged)\n",
        "    merged = Dropout(0.5)(merged)\n",
        "\n",
        "    merged = Dense(1)(merged)\n",
        "    output = Activation('sigmoid')(merged)\n",
        "\n",
        "    model = Model(inputs=[tweet_input], outputs=[output])\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "cnn_model = create_cnn_model()\n",
        "cnn_model.fit(xtrain, ytrain, epochs=10, batch_size=32, verbose=3)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import load_model\n",
        "cnn_model.save(\"CNN_HASOC.h5\")\n"
      ],
      "metadata": {
        "id": "klHmgAuKW3Xq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cjvqqld2R6WM"
      },
      "source": [
        "# Evaluating the model with test dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "QIVOjo-MR6WM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "800810ed-91a8-477e-d3cd-c5feab78fe25"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "250/250 [==============================] - 3s 12ms/step\n",
            "Accuracy\t0.8483483483483484\n",
            "Precision\t0.8374968458238709\n",
            "Recall\t0.8538718806277334\n",
            "f-measure\t0.8456050955414013\n",
            "cohen_kappa_score\t0.6966283583511558\n",
            "auc\t0.8484950146135013\n",
            "roc_auc\t0.8484950146135013\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import cohen_kappa_score\n",
        "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "p = cnn_model.predict(xtest,verbose=1)\n",
        "\n",
        "\n",
        "predicted = [int(round(x[0])) for x in p]\n",
        "predicted = np.array(predicted)\n",
        "actual = ytest\n",
        "\n",
        "tp = np.count_nonzero(predicted * actual)\n",
        "tn = np.count_nonzero((predicted - 1) * (actual - 1))\n",
        "fp = np.count_nonzero(predicted * (actual - 1))\n",
        "fn = np.count_nonzero((predicted - 1) * actual)\n",
        "\n",
        "\n",
        "\n",
        "accuracy = (tp + tn) / (tp + fp + fn + tn)\n",
        "precision = tp / (tp + fp)\n",
        "recall = tp / (tp + fn)\n",
        "fmeasure = (2 * precision * recall) / (precision + recall)\n",
        "cohen_kappa_score = cohen_kappa_score(predicted, actual)\n",
        "false_positive_rate, true_positive_rate, thresholds = roc_curve(actual, predicted)\n",
        "auc_val = auc(false_positive_rate, true_positive_rate)\n",
        "roc_auc_val = roc_auc_score(actual, predicted)\n",
        "\n",
        "print('Accuracy\\t' + str(accuracy))\n",
        "print('Precision\\t' + str(precision))\n",
        "print('Recall\\t' + str(recall))\n",
        "print('f-measure\\t' + str(fmeasure))\n",
        "print('cohen_kappa_score\\t' + str(cohen_kappa_score))\n",
        "print('auc\\t' + str(auc_val))\n",
        "print('roc_auc\\t' + str(roc_auc_val))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K7MkddJMR6WN"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing on HASOC test dataset"
      ],
      "metadata": {
        "id": "EZKn0yskf66l"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O1GqymwER6WN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f32ee589-ef5b-415d-f33d-21438ec51603"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "42/42 [==============================] - 1s 13ms/step\n",
            "Accuracy\t0.7185128983308042\n",
            "Precision\t0.6351039260969977\n",
            "Recall\t0.9090909090909091\n",
            "f-measure\t0.7477906186267844\n",
            "cohen_kappa_score\t0.4511526313308295\n",
            "auc\t0.7329465765650899\n",
            "roc_auc\t0.7329465765650899\n"
          ]
        }
      ],
      "source": [
        "\n",
        "X_test, y_test = load_data_and_labels_csv('/content/drive/MyDrive/IRE/hindi/hasoc_hi_t1_test.csv')\n",
        "ytest = np.array(y_test)\n",
        "xtest = tokenizer.texts_to_sequences(X_test)\n",
        "xtest = pad_sequences(xtest, maxlen=maxlen)\n",
        "\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "p = cnn_model.predict(xtest,verbose=1)\n",
        "\n",
        "\n",
        "predicted = [int(round(x[0])) for x in p]\n",
        "predicted = np.array(predicted)\n",
        "actual = ytest\n",
        "\n",
        "tp = np.count_nonzero(predicted * actual)\n",
        "tn = np.count_nonzero((predicted - 1) * (actual - 1))\n",
        "fp = np.count_nonzero(predicted * (actual - 1))\n",
        "fn = np.count_nonzero((predicted - 1) * actual)\n",
        "\n",
        "\n",
        "\n",
        "accuracy = (tp + tn) / (tp + fp + fn + tn)\n",
        "precision = tp / (tp + fp)\n",
        "recall = tp / (tp + fn)\n",
        "fmeasure = (2 * precision * recall) / (precision + recall)\n",
        "cohen_kappa_score = cohen_kappa_score(predicted, actual)\n",
        "false_positive_rate, true_positive_rate, thresholds = roc_curve(actual, predicted)\n",
        "auc_val = auc(false_positive_rate, true_positive_rate)\n",
        "roc_auc_val = roc_auc_score(actual, predicted)\n",
        "\n",
        "print('Accuracy\\t' + str(accuracy))\n",
        "print('Precision\\t' + str(precision))\n",
        "print('Recall\\t' + str(recall))\n",
        "print('f-measure\\t' + str(fmeasure))\n",
        "print('cohen_kappa_score\\t' + str(cohen_kappa_score))\n",
        "print('auc\\t' + str(auc_val))\n",
        "print('roc_auc\\t' + str(roc_auc_val))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing on MACD test dataset"
      ],
      "metadata": {
        "id": "qekxy_Fnf_Ss"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "X_test, y_test = load_data_and_labels_csv('/content/drive/MyDrive/IRE/hindi/macd_hi_test.csv')\n",
        "ytest = np.array(y_test)\n",
        "xtest = tokenizer.texts_to_sequences(X_test)\n",
        "xtest = pad_sequences(xtest, maxlen=maxlen)\n",
        "\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "p = cnn_model.predict(xtest,verbose=1)\n",
        "\n",
        "\n",
        "predicted = [int(round(x[0])) for x in p]\n",
        "predicted = np.array(predicted)\n",
        "actual = ytest\n",
        "\n",
        "tp = np.count_nonzero(predicted * actual)\n",
        "tn = np.count_nonzero((predicted - 1) * (actual - 1))\n",
        "fp = np.count_nonzero(predicted * (actual - 1))\n",
        "fn = np.count_nonzero((predicted - 1) * actual)\n",
        "\n",
        "\n",
        "\n",
        "accuracy = (tp + tn) / (tp + fp + fn + tn)\n",
        "precision = tp / (tp + fp)\n",
        "recall = tp / (tp + fn)\n",
        "fmeasure = (2 * precision * recall) / (precision + recall)\n",
        "cohen_kappa_score = cohen_kappa_score(predicted, actual)\n",
        "false_positive_rate, true_positive_rate, thresholds = roc_curve(actual, predicted)\n",
        "auc_val = auc(false_positive_rate, true_positive_rate)\n",
        "roc_auc_val = roc_auc_score(actual, predicted)\n",
        "\n",
        "print('Accuracy\\t' + str(accuracy))\n",
        "print('Precision\\t' + str(precision))\n",
        "print('Recall\\t' + str(recall))\n",
        "print('f-measure\\t' + str(fmeasure))\n",
        "print('cohen_kappa_score\\t' + str(cohen_kappa_score))\n",
        "print('auc\\t' + str(auc_val))\n",
        "print('roc_auc\\t' + str(roc_auc_val))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ew0v-L5fXuN",
        "outputId": "1a9e148d-fe80-44f9-98eb-84e278540ac9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "211/211 [==============================] - 3s 12ms/step\n",
            "Accuracy\t0.8307074910820452\n",
            "Precision\t0.8263638881196345\n",
            "Recall\t0.8535469107551488\n",
            "f-measure\t0.8397354720697904\n",
            "cohen_kappa_score\t0.6604366649379487\n",
            "auc\t0.8297746930013368\n",
            "roc_auc\t0.8297746930013368\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lUhc3uVFfXro"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YButWkM0fXpk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_model = load_model(\"network.h5\")\n",
        "loss, accuracy = loaded_model.evaluate(test_data, test_targets)"
      ],
      "metadata": {
        "id": "XCcaDp6rU4U_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lVEJYA12R6WN"
      },
      "outputs": [],
      "source": [
        "model_name = 'CNN'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1IF1TjMER6WN"
      },
      "outputs": [],
      "source": [
        "import datetime\n",
        "now = datetime.datetime.now()\n",
        "\n",
        "out_string = '=========='+str(now)+'==============\\n'\n",
        "#out_string += 'Language:\\t'+ lang+'\\n'\n",
        "out_string += 'Dataset:\\t' + dataset_name + '\\n'\n",
        "out_string += 'Task:\\t' + task + '\\n'\n",
        "out_string += str('Model Name:\\t' + model_name+'\\n')\n",
        "out_string += '-------------------------------------------------' + '\\n'\n",
        "\n",
        "out_string += 'Total Samples:\\t' + str(len(actual)) + '\\n'\n",
        "out_string += 'Positive Samples:\\t' + str(sum(actual)) + '\\n'\n",
        "out_string += 'Negative Samples:\\t' + str(len(actual)-sum(actual)) + '\\n'\n",
        "\n",
        "out_string += 'True Positive:\\t' + str(tp) + '\\n'\n",
        "out_string += 'True Negative:\\t' + str(tn) + '\\n'\n",
        "out_string += 'False Positive:\\t' + str(fp) + '\\n'\n",
        "out_string += 'False Negative:\\t' + str(fn) + '\\n'\n",
        "\n",
        "out_string += 'Accuracy:\\t' + str(accuracy) + '\\n'\n",
        "out_string += 'Precision:\\t' + str(precision) + '\\n'\n",
        "out_string += 'Recall:\\t' + str(recall) + '\\n'\n",
        "out_string += 'F-measure:\\t' + str(fmeasure) + '\\n'\n",
        "out_string += 'Cohen_Kappa_Score:\\t' + str(cohen_kappa_score) + '\\n'\n",
        "out_string += 'AUC:\\t' + str(auc_val) + '\\n'\n",
        "out_string += 'ROC_AUC:\\t' + str(roc_auc_val) + '\\n'\n",
        "out_string += '\\n'\n",
        "out_string += classification_report(actual, predicted)\n",
        "out_string += '\\n'\n",
        "print(out_string)\n",
        "with open(model_name+'.txt', 'a+') as FO:\n",
        "    FO.write(out_string)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}