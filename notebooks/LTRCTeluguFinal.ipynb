{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7010665,"sourceType":"datasetVersion","datasetId":4030703},{"sourceId":7011549,"sourceType":"datasetVersion","datasetId":4031317}],"dockerImageVersionId":30588,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install 'transformers[torch]'\n\n# Load model directly\nfrom transformers import AutoTokenizer, Trainer, TrainingArguments, AutoModelForSequenceClassification\nfrom torch.utils.data import Dataset\n\nimport torch\nimport numpy as np\nimport random\nfrom sklearn.metrics import accuracy_score, f1_score\ndef set_seeds(seed):\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    np.random.seed(seed)\n    random.seed(seed)\nset_seeds(42)\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\ntokenizer = AutoTokenizer.from_pretrained(\"ltrctelugu/bert_ltrc_telugu\")\nmodel = AutoModelForSequenceClassification.from_pretrained(\"ltrctelugu/bert_ltrc_telugu\")\nmodel.cuda()\n\ntrain_args = TrainingArguments(\n        'outputs',\n        evaluation_strategy = \"epoch\",\n        save_strategy = \"epoch\",\n        learning_rate = 2e-5,\n        per_device_train_batch_size = 20,\n        per_device_eval_batch_size = 20,\n        num_train_epochs = 10,\n        weight_decay = 0.01,\n        load_best_model_at_end = True,\n        metric_for_best_model = 'f1_macro'\n    )\n\nclass LTRCDataset(Dataset):\n    def __init__(self, text, labels, tokenizer, max_len):\n        self.text = text\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.text)\n\n    def __getitem__(self, item):\n        text = str(self.text[item])\n        label = self.labels[item]\n\n        encoding = self.tokenizer.encode_plus(\n            text,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            truncation=True,\n            padding='max_length',\n            return_token_type_ids=False,\n            return_attention_mask=True,\n            return_tensors='pt',\n        )\n\n        return {\n            'text': text,\n            'input_ids': encoding['input_ids'].flatten(),\n            'attention_mask': encoding['attention_mask'].flatten(),\n            'label': torch.tensor(label, dtype=torch.long)\n        }\n\n\nimport pandas as pd\ntrain_df = pd.read_csv('/kaggle/input/telugu/final_train.csv')\ntest_df = pd.read_csv('/kaggle/input/telugu/final_test.csv')\nval_df = pd.read_csv('/kaggle/input/telugu/final_val.csv')\n\ndef prepare_dataset(df):\n  return LTRCDataset(text=df.text.to_numpy(), labels=df.label_yn.to_numpy(), tokenizer=tokenizer, max_len=128)\n\ntrain_ds = prepare_dataset(train_df)\ntest_ds = prepare_dataset(test_df)\nval_ds = prepare_dataset(val_df)\n\ndef get_metrics(preds, labels):\n    acc = accuracy_score(labels, preds)\n    f1_micro = f1_score(labels, preds, average='micro')\n    f1_macro = f1_score(labels, preds, average='macro')\n    print ('jacc acc:{}, f1 micro score:{} f1 macro score:{}'.format(acc, f1_micro, f1_macro))\n    return acc, f1_micro, f1_macro\n\ndef compute_metrics(pred):\n    labels = pred.label_ids\n    preds = pred.predictions.argmax(-1)\n    acc, f1_micro, f1_macro = get_metrics(preds, labels)\n    print(f\"accuracy: {acc}, f1_macro: {f1_macro}, f1_micro: {f1_micro}\")\n    #return {'accuracy': acc, \"f1_macro\": f1_macro, \"f1_micro\": f1_micro}\n    return {'f1_macro':f1_macro, 'accuracy':acc}\n\ntrainer = Trainer(\n        model=model,\n        args=train_args,\n        train_dataset = train_ds,\n        eval_dataset = val_ds,\n        tokenizer = tokenizer,\n        compute_metrics = compute_metrics\n    )\n\ntrainer.train()\n\ntest_metrics = trainer.predict(test_ds)\n\ntest_metrics\n\nimport gc\ngc.collect()\ntorch.cuda.empty_cache()\n\n","metadata":{"execution":{"iopub.status.busy":"2023-11-20T13:43:29.188560Z","iopub.execute_input":"2023-11-20T13:43:29.189662Z","iopub.status.idle":"2023-11-20T15:27:17.932831Z","shell.execute_reply.started":"2023-11-20T13:43:29.189625Z","shell.execute_reply":"2023-11-20T15:27:17.931651Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: transformers[torch] in /opt/conda/lib/python3.10/site-packages (4.35.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (3.12.2)\nRequirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (0.17.3)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (1.24.3)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (2023.8.8)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (2.31.0)\nRequirement already satisfied: tokenizers<0.15,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (0.14.1)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (0.4.0)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (4.66.1)\nRequirement already satisfied: torch!=1.12.0,>=1.10 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (2.0.0)\nRequirement already satisfied: accelerate>=0.20.3 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (0.24.1)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.20.3->transformers[torch]) (5.9.3)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers[torch]) (2023.10.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers[torch]) (4.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers[torch]) (3.0.9)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers[torch]) (3.2.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers[torch]) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers[torch]) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers[torch]) (2023.7.22)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch!=1.12.0,>=1.10->transformers[torch]) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch!=1.12.0,>=1.10->transformers[torch]) (1.3.0)\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ltrctelugu/bert_ltrc_telugu and are newly initialized: ['classifier.bias', 'bert.pooler.dense.bias', 'classifier.weight', 'bert.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nYou're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='10650' max='10650' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [10650/10650 1:42:30, Epoch 10/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>F1 Macro</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.365800</td>\n      <td>0.498585</td>\n      <td>0.555474</td>\n      <td>0.705783</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.360500</td>\n      <td>0.477410</td>\n      <td>0.680512</td>\n      <td>0.733228</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.352600</td>\n      <td>0.474891</td>\n      <td>0.682132</td>\n      <td>0.734175</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.338700</td>\n      <td>0.497417</td>\n      <td>0.680185</td>\n      <td>0.737645</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.328600</td>\n      <td>0.510360</td>\n      <td>0.638982</td>\n      <td>0.732492</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.325000</td>\n      <td>0.530494</td>\n      <td>0.614188</td>\n      <td>0.726498</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.321300</td>\n      <td>0.553909</td>\n      <td>0.652166</td>\n      <td>0.733018</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.317500</td>\n      <td>0.552134</td>\n      <td>0.644630</td>\n      <td>0.732913</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>0.318600</td>\n      <td>0.563762</td>\n      <td>0.645805</td>\n      <td>0.731546</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.310000</td>\n      <td>0.573598</td>\n      <td>0.642268</td>\n      <td>0.730389</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"jacc acc:0.7057833859095689, f1 micro score:0.7057833859095689 f1 macro score:0.5554738452902812\naccuracy: 0.7057833859095689, f1_macro: 0.5554738452902812, f1_micro: 0.7057833859095689\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"name":"stdout","text":"jacc acc:0.7332281808622503, f1 micro score:0.7332281808622502 f1 macro score:0.6805121462780512\naccuracy: 0.7332281808622503, f1_macro: 0.6805121462780512, f1_micro: 0.7332281808622502\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"name":"stdout","text":"jacc acc:0.7341745531019979, f1 micro score:0.7341745531019979 f1 macro score:0.6821323857116541\naccuracy: 0.7341745531019979, f1_macro: 0.6821323857116541, f1_micro: 0.7341745531019979\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"name":"stdout","text":"jacc acc:0.7376445846477392, f1 micro score:0.7376445846477392 f1 macro score:0.6801848533421222\naccuracy: 0.7376445846477392, f1_macro: 0.6801848533421222, f1_micro: 0.7376445846477392\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"name":"stdout","text":"jacc acc:0.7324921135646688, f1 micro score:0.7324921135646688 f1 macro score:0.6389820113557514\naccuracy: 0.7324921135646688, f1_macro: 0.6389820113557514, f1_micro: 0.7324921135646688\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"name":"stdout","text":"jacc acc:0.7264984227129337, f1 micro score:0.7264984227129337 f1 macro score:0.6141884869760845\naccuracy: 0.7264984227129337, f1_macro: 0.6141884869760845, f1_micro: 0.7264984227129337\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"name":"stdout","text":"jacc acc:0.7330178759200842, f1 micro score:0.7330178759200842 f1 macro score:0.6521661027509125\naccuracy: 0.7330178759200842, f1_macro: 0.6521661027509125, f1_micro: 0.7330178759200842\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"name":"stdout","text":"jacc acc:0.732912723449001, f1 micro score:0.732912723449001 f1 macro score:0.6446298135275295\naccuracy: 0.732912723449001, f1_macro: 0.6446298135275295, f1_micro: 0.732912723449001\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"name":"stdout","text":"jacc acc:0.7315457413249211, f1 micro score:0.7315457413249211 f1 macro score:0.6458049352635246\naccuracy: 0.7315457413249211, f1_macro: 0.6458049352635246, f1_micro: 0.7315457413249211\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"name":"stdout","text":"jacc acc:0.7303890641430074, f1 micro score:0.7303890641430074 f1 macro score:0.642267725130095\naccuracy: 0.7303890641430074, f1_macro: 0.642267725130095, f1_micro: 0.7303890641430074\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"jacc acc:0.792373206475869, f1 micro score:0.792373206475869 f1 macro score:0.6965803960948703\naccuracy: 0.792373206475869, f1_macro: 0.6965803960948703, f1_micro: 0.792373206475869\n","output_type":"stream"}]},{"cell_type":"code","source":"test_metrics = trainer.predict(test_ds)","metadata":{"execution":{"iopub.status.busy":"2023-11-20T15:36:13.221933Z","iopub.execute_input":"2023-11-20T15:36:13.222312Z","iopub.status.idle":"2023-11-20T15:37:16.624066Z","shell.execute_reply.started":"2023-11-20T15:36:13.222282Z","shell.execute_reply":"2023-11-20T15:37:16.622730Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"jacc acc:0.792373206475869, f1 micro score:0.792373206475869 f1 macro score:0.6965803960948703\naccuracy: 0.792373206475869, f1_macro: 0.6965803960948703, f1_micro: 0.792373206475869\n","output_type":"stream"}]},{"cell_type":"code","source":"original_test_df = pd.read_csv('/kaggle/input/ltrctest/ltrc_tel_test.csv')\noriginal_test_ds = prepare_dataset(original_test_df)\noriginal_test_metrics = trainer.predict(original_test_ds)","metadata":{"execution":{"iopub.status.busy":"2023-11-20T15:38:39.783470Z","iopub.execute_input":"2023-11-20T15:38:39.783881Z","iopub.status.idle":"2023-11-20T15:39:14.455939Z","shell.execute_reply.started":"2023-11-20T15:38:39.783849Z","shell.execute_reply":"2023-11-20T15:39:14.454872Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"jacc acc:0.9522252239442628, f1 micro score:0.9522252239442628 f1 macro score:0.48776402039329936\naccuracy: 0.9522252239442628, f1_macro: 0.48776402039329936, f1_micro: 0.9522252239442628\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}